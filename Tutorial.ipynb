{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before doing anything, please run the following cell which will make a CSV file called \"data.csv\" in this directory and which creates a local spark session.\n",
    "\n",
    "## TODO: \n",
    "1. Put some NaNs into the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "\n",
    "# Creates data\n",
    "!python ./data_setup.py  \n",
    "\n",
    "# Creates a local spark session.\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PySpark Tutorial\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Instance and Getting the Data\n",
    "\n",
    "Here we're going to make a Spark Context.  If you need to restart it, go into \"Kernel > Restart Kernel.\"  In general, if anything bad happens spark-related, you're going to want to do that reset kernel thing.\n",
    "\n",
    "Problems will be given by section as below.  Note that you *will have to import some new modules from pyspark; not all required imports are given above*.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Importing with inferSchema\n",
    "\n",
    "Import the `data.csv` file.  Use `inferSchema=True` to infer the schema.  Be sure that you can print the head of the rdd as well as printing out the schema using `.printSchema()`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Importing with Structs\n",
    "\n",
    "Import the `data.csv` file (again).  Explicitly use StructFields to create the schema.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3  Some Example Queries of the Data\n",
    "\n",
    "1. Select only those values whose `categorical_col` is `Low` and the `int_col` value is negative.\n",
    "2. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns.\n",
    "3. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns, and show only the ones having an average greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this stuff after this\n",
    "\n",
    "file_loc = \"./data.csv\"\n",
    "rdd = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .load(file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|categorical_col|count|\n",
      "+---------------+-----+\n",
      "|           High| 2720|\n",
      "|            Low|13149|\n",
      "|         Medium| 5231|\n",
      "|            Off| 5181|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only datetime_col, float_col, categorical_col.\n",
    "# Print 25 records without Python truncating the dataset\n",
    "# (hint, you will use a parameter in the .show() method.)\n",
    "\n",
    "# cols = ['datetime_col', 'float_col', 'categorical_col']\n",
    "# rdd.select(*cols) \\\n",
    "#     .show(25, truncate=False)\n",
    "\n",
    "\n",
    "# Use the describe method to see some summary statistics for float_col, int_col.\n",
    "# rdd.describe()\n",
    "\n",
    "\n",
    "# Count the number of times each category comes up in the categorical column.\n",
    "# rdd.select(\"categorical_col\") \\\n",
    "#     .groupBy(\"categorical_col\") \\\n",
    "#     .count() \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, float_col: string, int_col: string, categorical_col: string, int_col2: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casting\n",
    "\n",
    "# Cast integer column as DoubleType and check the schema.\n",
    "# (Cast them back when you're done!)\n",
    "from pyspark.sql.types import DoubleType\n",
    "rdd1 = rdd.withColumn(\"int_col2\", rdd[\"int_col\"].cast(DoubleType()))\n",
    "\n",
    "# TODO: Why is this showing all strings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML is next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
