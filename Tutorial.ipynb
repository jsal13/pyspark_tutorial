{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before doing anything, please run the following cell which will make a CSV file called \"data.csv\" in this directory.\n",
    "\n",
    "## TODO: \n",
    "1. Put some NaNs into the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "!python ./data_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Instance and Getting the Data\n",
    "\n",
    "Here we're going to make a Spark Context.  If you need to restart it, go into \"Kernel > Restart Kernel.\"  In general, if anything bad happens spark-related, you're going to want to do that reset kernel thing.\n",
    "\n",
    "Problems will be given by section as below.  Note that you *will have to import some new modules from pyspark; not all required imports are given above*.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1\n",
    "\n",
    "Import the `data.csv` file.  What types are the columns?\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2\n",
    "\n",
    "Edit the code to include the column types from the CSV.  You should know how to do this \"automatically\" using a function argument as well as manually using structs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3\n",
    "\n",
    "1. Select only those values whose `categorical_col` is `High`.\n",
    "2. Select only those values whose `categorical_col` is `Low` and the `int_col` value is negative.\n",
    "3. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns.\n",
    "4. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns, and show only the ones having an average greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Tutorial\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
