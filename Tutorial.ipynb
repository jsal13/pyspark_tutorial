{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before doing anything, please run the following cell which will make a CSV file called \"data.csv\" in this directory.\n",
    "\n",
    "## TODO: \n",
    "1. Put some NaNs into the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "!python ./data_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Instance and Getting the Data\n",
    "\n",
    "Here we're going to make a Spark Context.  If you need to restart it, go into \"Kernel > Restart Kernel.\"  In general, if anything bad happens spark-related, you're going to want to do that reset kernel thing.\n",
    "\n",
    "Problems will be given by section as below.  Note that you *will have to import some new modules from pyspark; not all required imports are given above*.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Importing with inferSchema\n",
    "\n",
    "Import the `data.csv` file.  Use `inferSchema=True` to infer the schema.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Importing with Structs\n",
    "\n",
    "Import the `data.csv` file (again).  Explicitly use StructFields to create the schema.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3  Some Example Queries of the Data\n",
    "\n",
    "1. Select only those values whose `categorical_col` is `Low` and the `int_col` value is negative.\n",
    "2. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns.\n",
    "3. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns, and show only the ones having an average greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = \"./data.csv\"\n",
    "rdd = spark.read.format(\"csv\") \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .load(file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------+---------------+--------+\n",
      "|       datetime_col|           float_col|int_col|categorical_col|bool_col|\n",
      "+-------------------+--------------------+-------+---------------+--------+\n",
      "|2018-01-01 00:00:00|  0.5748556785619214|    -99|            Low|    true|\n",
      "|2018-01-01 05:00:00|  0.7361655134141251|    -82|            Low|    true|\n",
      "|2018-01-02 03:00:00|  0.9940334727502934|    -99|            Low|   false|\n",
      "|2018-01-02 04:00:00|   0.726596701630739|     -8|            Low|   false|\n",
      "|2018-01-02 10:00:00| 0.20751173210042095|    -61|            Low|   false|\n",
      "|2018-01-02 13:00:00|  0.6195021839986666|    -12|            Low|   false|\n",
      "|2018-01-02 14:00:00|  0.7408211423975096|    -17|            Low|   false|\n",
      "|2018-01-02 22:00:00|   0.976770375639157|    -29|            Low|   false|\n",
      "|2018-01-03 03:00:00|   0.774084033537829|    -14|            Low|   false|\n",
      "|2018-01-03 06:00:00|   0.808864300373038|    -90|            Low|   false|\n",
      "|2018-01-03 07:00:00| 0.24417454752848822|    -23|            Low|    true|\n",
      "|2018-01-03 09:00:00| 0.47285457774947914|    -88|            Low|   false|\n",
      "|2018-01-03 10:00:00|  0.4844796582782044|    -93|            Low|    true|\n",
      "|2018-01-03 14:00:00|  0.6486714618466595|    -67|            Low|    true|\n",
      "|2018-01-03 15:00:00|  0.3503582050032772|    -43|            Low|    true|\n",
      "|2018-01-03 17:00:00|  0.5968161298702995|    -13|            Low|   false|\n",
      "|2018-01-03 22:00:00|  0.6469954654660127|    -19|            Low|    true|\n",
      "|2018-01-04 07:00:00| 0.22471246246025356|    -40|            Low|   false|\n",
      "|2018-01-04 10:00:00|0.037641189031790034|    -61|            Low|   false|\n",
      "|2018-01-04 17:00:00|  0.9106328050765001|    -64|            Low|   false|\n",
      "+-------------------+--------------------+-------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.filter(rdd.categorical_col == \"Low\") \\\n",
    "   .filter(rdd.int_col < 0) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
