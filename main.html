<html>
  <head>
    <title>PySpark Tutorial</title>
    <!-- <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/themes/prism-twilight.min.css"
      rel="stylesheet"
    /> -->
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/themes/prism-tomorrow.min.css"
      rel="stylesheet"
    />
  </head>

  <body>
    <h2>Setup</h2>
    <p>
      <b>First,</b> build the docker image with the following in your terminal:
    </p>

    <pre><code class="language-bash">docker build -t pyspark_tutorial .</code></pre>

    <p>
      <b>Second,</b> after the build is complete, we'll run the Jupyter server
      to bring up our PySpark Jupyter notebook.
    </p>

    <pre><code class="language-bash">docker run -v /path/to/dir/you/save/work/to:/home/jovyan/work -p 8888:8888 pyspark_tutorial</code></pre>

    <p>
      <b>Third,</b> we're going to copy the link in the terminal which will look
      something like this:
    </p>

    <pre><code class="language-bash">http://127.0.0.1:8888/?token=lotsandlotsofnumbersandletters</code></pre>

    <p>
      <b>Last,</b> navigate to the "Tutorial" notebook and enter it. Run the
      "Setup" section and then follow the instructions! Solutions to portions of
      the code will be given here with their associated section number.
    </p>

    <br />
    <hr />
    <hr />
    <br />

    <h1>Solutions.</h1>

    Important note here!

    <pre><code class="language-python"># IMPORTANT NOTE: We're going to use functions here a bunch, and the typical
# import alias is F.
import pyspark.sql.functions as F</code></pre>

    <h2>Section 1</h2>
    <pre><code class="language-python"># 1.1
# We can use inferSchema to infer the schema of the CSV.
rdd = spark.read.csv(path="./data.csv", 
                     header=True, 
                     inferSchema=True)

# 1.2
# We can use structs explicitly to construct the schema.
# It's more annoying, but you know exactly what you're getting.
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    TimestampType, IntegerType, BooleanType, 
    DoubleType
)

schema = StructType([
    StructField('datetime_col', TimestampType(), True),
    StructField('float_col', DoubleType(), True),
    StructField('int_col', IntegerType(), True),
    StructField('categorical_col', StringType(), True),
    StructField('bool_col', BooleanType(), True),
])
rdd = spark.read.csv(path="./data.csv", 
                     header=True, 
                     schema=schema)

# 1.3.1
rdd.filter(col('categorical_col') == "High").show()

# 1.3.2
# I like to separate the mask out if it gets too long.
mask = ((col('categorical_col') == "Low") & 
       (col('int_col') < 0))
rdd.filter(mask).show()

# 1.3.3
# Here's one potential way to do aggregations.
rdd.groupBy("categorical_col").agg({"int_col": "average"}).show()

# 1.3.4
# I like to split out the aggregations.
# Note that we have given aliases to these columns.
exprs = [F.mean(col("int_col")).alias("int_col_mean"),
         F.sum(col("int_col")).alias("int_col_sum")]
rdd.groupBy("categorical_col") \
    .agg(*exprs) \
    .where(col("int_col_mean") > 0) \
    .show()
</code></pre>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/plugins/autoloader/prism-autoloader.min.js"></script>
  </body>
</html>
